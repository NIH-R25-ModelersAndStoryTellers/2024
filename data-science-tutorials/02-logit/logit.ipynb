{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: Predictive Modeling - Logistic Regression and Regularization\n",
        "author: 'Dr. Roch Nianogo, Bowen Zhang, Dr. Hua Zhou'\n",
        "format:\n",
        "  html:\n",
        "    theme: cosmo\n",
        "    embed-resources: true\n",
        "    number-sections: true\n",
        "    toc: true\n",
        "    toc-depth: 4\n",
        "    toc-location: left\n",
        "    code-fold: false\n",
        "    link-external-icon: true\n",
        "    link-external-newwindow: true\n",
        "knitr:\n",
        "  opts_chunk:\n",
        "    cache: false\n",
        "    echo: true\n",
        "    fig.align: center\n",
        "    message: false\n",
        "comments:\n",
        "  hypothesis: true\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "#| output: false\n",
        "\n",
        "library(tidyverse)\n",
        "library(tidycensus)\n",
        "census_api_key(\"4cf445b70eabd0b297a45e7f62f52c27ba3b5cae\",\n",
        "               install = TRUE, overwrite = TRUE)\n",
        "Sys.setenv(\"CENSUS_KEY\" = \"4cf445b70eabd0b297a45e7f62f52c27ba3b5cae\")\n",
        "library(censusapi)\n",
        "library(gtsummary)\n",
        "library(knitr)\n",
        "library(tigris)\n",
        "library(maps)\n",
        "library(ggplot2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"text-align: center;\">\n",
        "  <img src=\"https://d33wubrfki0l68.cloudfront.net/795c039ba2520455d833b4034befc8cf360a70ba/558a5/diagrams/data-science-explore.png\" alt=\"Data Science Diagram\">\n",
        "</div>\n",
        "\n",
        "\n",
        "## Logistic Regression\n",
        "\n",
        "### Brief review of linear regression\n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        "  <img src=\"https://www.frontiersin.org/files/Articles/815717/fbuil-08-815717-HTML/image_m/fbuil-08-815717-g003.jpg\" alt=\"Machine Learning\">\n",
        "</div>\n",
        "\n",
        "-   `Default` dataset\n",
        "\n",
        "We are interested in predicting whether an individual will default on his or her credit card payment, on the basis of annual income and monthly credit card balance.\n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        "  <img src=\"./4_1.jpg\" alt=\"default\" width=700>\n",
        "</div>\n",
        "\n",
        "The response `default` falls into one of two categories, Yes or No. Rather than modeling this response Y directly, logistic regression models the probability that Y belongs to a particular category.\n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        "  <img src=\"./4_2.jpg\" alt=\"default\" width=700>\n",
        "</div>\n",
        "\n",
        "$$\n",
        "Y_i = \\begin{cases}\n",
        "1 & \\text{with probability } p_i \\\\\n",
        "0 & \\text{with probability } 1 - p_i\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "The parameter $p_i = \\mathbb{E}(Y_i)$ will be related to the predictors $\\mathbf{x}_i$ via\n",
        "\n",
        "$$\n",
        "p_i = \\frac{e^{\\eta_i}}{1 + e^{\\eta_i}},\n",
        "$$\n",
        "where $\\eta_i$ is the **linear predictor** or **systematic component**\n",
        "$$\n",
        "\\eta_i = \\mathbf{x}_i^T \\boldsymbol{\\beta}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\log \\left( \\frac{p}{1-p} \\right) = \\log(\\text{odds}) = \\beta_0 + \\beta_1  x_1 + \\beta_2  x_2 + \\dots + \\beta_q  x_q. \n",
        "$$\n",
        "\n",
        "Therefore $\\beta_1$ can be interpreted as a unit increase in $x_1$ with other predictors held fixed increases the **log-odds** of success by $\\beta_1$, or increase the odds of success by a factor of $e^{\\beta_1}$. \n",
        "\n",
        "### Logistic Regression on Food Security Data\n",
        "\n",
        "To further investigate the factors that are associated with food insecurity, we can use logistic regression to model the probability of a household being in low food security.\n",
        "\n",
        "We include the following independent variables in the model:\n",
        "\n",
        "-   HRNUMHOU: Total number of persons living in the household.\n",
        "-   HRHTYPE: Household type\n",
        "    -   0 = Non-interview household\n",
        "    -   1 = Husband/wife primary family (neither AF)\n",
        "    -   2 = Husband/wife primary family (either/both AF)\n",
        "    -   3 = Unmarried civilian male-primary. family householder\n",
        "    -   4 = Unmarried civilian female-prim family householder\n",
        "    -   5 = Primary family householder-rp in AF, unmarried.\n",
        "    -   6 = Civilian male primary individual\n",
        "    -   7 = Civilian female primary individual\n",
        "    -   8 = Primary individual householder-rp in AF\n",
        "    -   9 = Group quarters with family\n",
        "    -   10 = Group quarters without family\n",
        "-   GEREG: Region\n",
        "    -   1 = Northeast\n",
        "    -   2 = Midwest\n",
        "    -   3 = South\n",
        "    -   4 = West\n",
        "-   PRTAGE: Age of the reference person\n",
        "-   PEHSPNON: Hispanic or non-hispanic of the reference person\n",
        "    -   1 = Hispanic\n",
        "    -   2 = Non-Hispanic\n",
        "-   PTDTRACE: Race of the reference person\n",
        "    -   1 = White only\n",
        "    -   2 = Black only\n",
        "    -   3\\~26 = Others\n",
        "-   PEEDUCA: Highest level of education completed\n",
        "    -   31 = Less than 1st grade\n",
        "    -   32 = 1st, 2nd, 3rd, or 4th grade\n",
        "    -   33 = 5th or 6th grade\n",
        "    -   34 = 7th or 8th grade\n",
        "    -   35 = 9th grade\n",
        "    -   36 = 10th grade\n",
        "    -   37 = 11th grade\n",
        "    -   38 = 12th grade no diploma\n",
        "    -   39 = High school grad-diploma or equiv (GED)\n",
        "    -   40 = Some college but no degree\n",
        "    -   41 = Associate degree-occupational/vocational\n",
        "    -   42 = Associate degree-academic program\n",
        "    -   43 = Bachelor's degree (ex: BA, AB, BS)\n",
        "    -   44 = Master's degree (ex: MA, MS, MEng, MEd, MSW)\n",
        "    -   45 = Professional school degree (ex: MD, DDS, DVM)\n",
        "    -   46 = Doctorate degree (ex: PhD, EdD)\n",
        "-   PEMLR: Employment status\n",
        "    -   1 = Employed-at work\n",
        "    -   2 = Employed-absent\n",
        "    -   3 = Unemployed-on layoff\n",
        "    -   4 = Unemployed-looking\n",
        "    -   5 = Not in labor force-retired\n",
        "    -   6 = Not in labor force-disabled\n",
        "    -   7 = Not in labor force-other\n",
        "-   PRCHLD: Presence of own children < 18 years old\n",
        "    -   0 = No children\n",
        "    -   1~15 = Have children\n",
        "-   HRPOOR: Poverty status\n",
        "    -   1 = Poor (Below 185% poverty)\n",
        "    -   2 = Not poor (Above 185% poverty or income not reported)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "\n",
        "fss_21_data <- censusapi::getCensus(name = \"cps/foodsec/dec\",\n",
        "                                    vintage = 2021,\n",
        "                                    vars = c(\"HRHHID\", \"HRHHID2\", \"HRNUMHOU\",\n",
        "                                             \"HRHTYPE\", \"GEREG\", \"HRPOOR\",\n",
        "                                             \"PRTAGE\", \"PEEDUCA\", \"PRCHLD\",\n",
        "                                             \"PEMLR\", \"PTDTRACE\", \"PEHSPNON\",\n",
        "                                             \"HRFS12M1\", \"HHSUPWGT\",\n",
        "                                             \"PERRP\")) |>\n",
        "  as_tibble() |>\n",
        "  filter(PERRP %in% c(40, 41), # Reference person\n",
        "         HRFS12M1 %in% c(1, 2, 3), # Valid food security status codes\n",
        "         PEMLR %in% c(1, 2, 3, 4, 5, 6, 7), # Valid employment status codes\n",
        "         ) |>\n",
        "  \n",
        "  # Keep householder of the family\n",
        "  group_by(HRHHID, HRHHID2) |>\n",
        "  slice_min(PERRP) |>\n",
        "  ungroup() |>\n",
        "  \n",
        "  # Data preprocessing\n",
        "  mutate(HRFS12M1 = factor(HRFS12M1,\n",
        "                           levels = c(\"1\", \"2\", \"3\"),\n",
        "                           labels = c(\"Food Security\",\n",
        "                                      \"Low Food Security\",\n",
        "                                      \"Very Low Food Security\")),\n",
        "         HRNUMHOU = as.numeric(HRNUMHOU),\n",
        "         HHSUPWGT = as.numeric(HHSUPWGT),\n",
        "         \n",
        "         # Combining some categories\n",
        "         HRHTYPE = cut(as.numeric(HRHTYPE), breaks = c(0, 3, 6, 9, 10),\n",
        "                       labels = c(\"MarriedFamily\", \"UnmarriedFamily\",\n",
        "                                  \"Individual\", \"GroupQuarters\")),\n",
        "         GEREG = factor(GEREG, levels = c(1, 2, 3, 4),\n",
        "                        labels = c(\"Northeast\", \"Midwest\", \"South\", \"West\")),\n",
        "         PEEDUCA = cut(as.numeric(PEEDUCA), breaks = c(30, 39, 43, 46),\n",
        "                       labels = c(\"LessThanHighSchool\",\n",
        "                                  \"HighSchoolOrAssociateDegree\",\n",
        "                                  \"CollegeOrHigher\")),\n",
        "         PEMLR = cut(as.numeric(PEMLR), breaks = c(0, 3, 5, 7),\n",
        "                     labels = c(\"Employed\", \"NotEmployed\",\n",
        "                                \"NotInLaborForce\")),\n",
        "         \n",
        "         PRTAGE = as.numeric(PRTAGE),\n",
        "         PEHSPNON = factor(PEHSPNON, levels = c(1, 2),\n",
        "                           labels = c(\"Hispanic\", \"Non-Hispanic\")),\n",
        "         PTDTRACE = cut(as.numeric(PTDTRACE), breaks = c(0, 2, 3, 26),\n",
        "                        labels = c(\"White\", \"Black\", \"Others\")),\n",
        "         RACE = ifelse(PEHSPNON == \"Hispanic\",\n",
        "                       \"Hispanic\", str_c(PTDTRACE, \" non-Hispanic\")),\n",
        "         PRCHLD = ifelse(PRCHLD == 0, \"NoChildren\", \"Children\"),\n",
        "         HRPOOR = factor(HRPOOR, levels = c(1, 2),\n",
        "                         labels = c(\"Poor\", \"NotPoor\"))\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To transform the response variable `HRFS12M1` into a binary variable, we set `HRFS12M1 = 0` if the household is in food security and `HRFS12M1 = 1` otherwise.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "\n",
        "data_clean <- fss_21_data |>\n",
        "  select(HRFS12M1, HRNUMHOU, HRHTYPE, GEREG, PRCHLD, HRPOOR,\n",
        "         PRTAGE, PEEDUCA, PEMLR, RACE, HHSUPWGT) |>\n",
        "  mutate(HRFS12M1 = ifelse(HRFS12M1 == \"Food Security\", 0, 1))\n",
        "\n",
        "logit_model <- glm(HRFS12M1 ~ . - HHSUPWGT,\n",
        "                   data = data_clean, family = \"binomial\")\n",
        "\n",
        "# gtsummary package offers a nice summary of the model\n",
        "logit_model |>\n",
        "  tbl_regression() |>\n",
        "  bold_labels() |>\n",
        "  bold_p(t = 0.05)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ROC Curve\n",
        "\n",
        "If we want to use this model for prediction, we need to evaluate its performance. There are many metrics related to classification models, such as accuracy, precision, recall, Sensitivity, Specificity,...\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\text{Accuracy} &= \\frac{TP + TN}{TP + TN + FP + FN} \\\\\n",
        "\\text{Precision} &= \\frac{TP}{TP + FP} \\\\\n",
        "\\text{Recall} &= \\frac{TP}{TP + FN} \\\\\n",
        "\\text{Sensitivity} &= \\frac{TP}{TP + FN} \\\\\n",
        "\\text{Specificity} &= \\frac{TN}{TN + FP}\n",
        "\\end{aligned}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "\n",
        "pred_prob <- predict(logit_model, type = \"response\")\n",
        "\n",
        "# Set the threshold 0.5\n",
        "threshold <- 0.5\n",
        "\n",
        "predicted_class <- ifelse(pred_prob > threshold, 1, 0)\n",
        "predicted_class <- as.factor(predicted_class)\n",
        "actual_class <- data_clean$HRFS12M1\n",
        "\n",
        "ctb_50 <- table(Predicted = predicted_class, Actual = actual_class)\n",
        "\n",
        "# Set the threshold 0.1\n",
        "threshold <- 0.1\n",
        "\n",
        "predicted_class <- ifelse(pred_prob > threshold, 1, 0)\n",
        "predicted_class <- as.factor(predicted_class)\n",
        "actual_class <- data_clean$HRFS12M1\n",
        "\n",
        "ctb_10 <- table(Predicted = predicted_class, Actual = actual_class)\n",
        "\n",
        "list(threshold50 = ctb_50, threshold10 = ctb_10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we set different thresholds, the confusion matrix will change, thus the metrics will change."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "\n",
        "# Calculate metrics under different thresholds\n",
        "calc_metrics <- function(ct) {\n",
        "  c(Accuracy = (ct[1, 1] + ct[2, 2]) / sum(ct),\n",
        "    Sensitivity = ct[2, 2] / sum(ct[, 2]),\n",
        "    Specificity = ct[1, 1] / sum(ct[, 1]),\n",
        "    Precision = ct[2, 2] / sum(ct[2, ]))\n",
        "}\n",
        "\n",
        "\n",
        "metrics50 <- calc_metrics(ctb_50)\n",
        "metrics10 <- calc_metrics(ctb_10)\n",
        "\n",
        "data.frame(threshold50 = metrics50, threshold10 = metrics10) |>\n",
        "  t() |>\n",
        "  round(2) |>\n",
        "  kable()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we set the threshold to 0.5, the accuracy is 0.90, which is quite higher than setting threshold to 0.1. However, the sensitivity is only 0.02, which means the model can only capture 2% of the households in low food security. In contrast, if we set the threshold to 0.1, the sensitivity is 0.69, which means the model can capture 68% of the households in low food security. However, the specificity decreases to 0.76. This trade-off is common in classification models.\n",
        "\n",
        "Therefore, we need a metric that can evaluate the model's performance under different thresholds. The ROC curve is a good choice. The ROC curve is a popular graphic for simultaneously displaying the two types of errors for all possible thresholds. The name “ROC” is historic, and comes from communications theory. It is an acronym for receiver operating characteristics.\n",
        "\n",
        "The overall performance of a classifier, summarized over all possible thresholds, is given by the area under the (ROC) curve (AUC). An ideal ROC curve will hug the top left corner, so the larger area under the AUC the better the classifier. We expect a classifier that performs no better than chance to have an AUC of 0.5.\n",
        "\n",
        "There is another similar plot called the precision-recall curve, which sets the x-axis as recall and the y-axis as precision. The classifier that has a higher AUC on the ROC curve will always have a higher AUC on the PR curve as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "\n",
        "library(pROC)\n",
        "\n",
        "data_clean <- data_clean |>\n",
        "  mutate(prob = predict(logit_model, type = \"response\"))\n",
        "\n",
        "roc_data <- roc(data_clean$HRFS12M1, data_clean$prob)\n",
        "\n",
        "ggroc(roc_data, legacy.axes = TRUE) +\n",
        "  labs(title = \"ROC Curve for Logistic Regression Model\",\n",
        "       x = \"1 - Specificity\",\n",
        "       y = \"Sensitivity\") +\n",
        "  theme_minimal() +\n",
        "  annotate(\"text\", x = 0.5, y = 0.5,\n",
        "           label = paste(\"AUC =\", round(auc(roc_data), 3)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The logistic regression model has an AUC of 0.788, which indicates that the model has a good discrimination ability. However, if we want to evaluate the model's predictive performance, simply fitting models and calculating AUC is not enough.\n",
        "\n",
        "## Assessing Model Accuracy\n",
        "\n",
        "### Measuring the Quality of Fit (ISL 2.2)\n",
        "\n",
        "In order to evaluate the performance of a statistical learning method on a given data set, we need some way to measure how well its predictions actually match the observed data. That is, we need to quantify the extent to which the predicted response value for a given observation is close to the true response value for that observation. In the regression setting, the most commonly-used measure is the mean squared error (MSE), given by\n",
        "\n",
        "$$\n",
        "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{f}(x_i))^2.\n",
        "$$\n",
        "The MSE will be small if the predicted responses are very close to the true responses, and will be large if for some of the observations, the predicted and true responses differ substantially.\n",
        "\n",
        "The MSE is computed using the training data that was used to fit the model. But in general, we do not really care how well the method works training on the training data. Rather, we are interested in the accuracy of the predictions that we obtain when we apply our method to previously unseen test data. We’d like to select the model for which the average of the test MSE is as small as possible.\n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        "  <img src=\"./2_9.jpg\" alt=\"default\" width=700>\n",
        "</div>\n",
        "\n",
        "As model flexibility increases, training MSE will decrease, but the test MSE may not. When a given method yields a small training MSE but a large test MSE, we are said to be overfitting the data.\n",
        "\n",
        "\n",
        "::: {.callout-tip}\n",
        "-   Does it mean simpler models are always better?\n",
        "\n",
        "**No Free Lunch Theorem** [David Wolpert, William Macready]: Any two optimization algorithms are equivalent when their performance is averaged across all possible problems.\n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        "  <img src=\"./nfl_train.jpg\" alt=\"default\" width=700>\n",
        "</div>\n",
        "\n",
        "The black points represent the training data. There are two models A and B, in which model B is more flexible than model A. \n",
        "\n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        "  <img src=\"./nfl_test.jpg\" alt=\"default\" width=700>\n",
        "</div>\n",
        "\n",
        "The white points represent the test data. In the left panel, model A has a smaller test MSE than model B. In the right panel, model B has a smaller test MSE than model A. Therefore, we cannot say that simpler models are always better.\n",
        ":::\n",
        "\n",
        "\n",
        "In practice, one can usually compute the training MSE with relative ease, but estimating test MSE is considerably more difficult because usually no test data are available. The flexibility level corresponding to the model with the minimal test MSE can vary considerably among data sets. One important method is cross-validation, which is a cross method for estimating test MSE using the training data.\n",
        "\n",
        "### Cross-Validation (ISL 5.1)\n",
        "\n",
        "K-fold cross-validation randomly divides the set of observations into k groups, or folds, of approximately equal size. The first fold is treated as a validation set, and the method is fit on the remaining k − 1 folds. The mean squared error, $\\text{MSE}_1$, is then computed on the observations in the held-out fold. This procedure is repeated k times; each time, a different group of observations is treated as a validation set. This process results in k estimates of the test error, $\\text{MSE}_1$, $\\text{MSE}_2$,..., $\\text{MSE}_k$. The k-fold CV estimate is computed by averaging these values,\n",
        "\n",
        "$$\n",
        "\\text{CV}_{(k)} = \\frac{1}{k} \\sum_{i=1}^{k} \\text{MSE}_i.\n",
        "$$\n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        "  <img src=\"./kfold.jpg\" alt=\"default\" width=700>\n",
        "</div>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Tidymodels Overview\n",
        "\n",
        "- [tidymodels](https://www.tidymodels.org/) is an ecosystem for:\n",
        "\n",
        "    1. Feature engineering: coding qualitative predictors, transformation of predictors (e.g., log), extracting key features from raw variables (e.g., getting the day of the week out of a date variable), interaction terms, ... ([recipes](https://recipes.tidymodels.org/reference/index.html) package);  \n",
        "    2. Build and fit a model ([parsnip](https://parsnip.tidymodels.org/index.html) package);  \n",
        "    3. Evaluate model using resampling (such as cross-validation) ([tune](https://tune.tidymodels.org/) and [dial](https://dials.tidymodels.org/) packages);  \n",
        "    4. Tuning model parameters.\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://rviews.rstudio.com/2020/04/21/the-case-for-tidymodels/tidymodels.png\" height=\"300\">\n",
        "</p>\n",
        "\n",
        "- tidymodels is the R analog of [sklearn.pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) in Python and [MLJ.jl](https://juliaai.github.io/MLJ.jl/stable/) in Julia.\n",
        "\n",
        "\n",
        "## Elastic Net Regularization (Shrinkage Methods, ISL 6.2)\n",
        "\n",
        "The subset selection methods such as best subset selection, forward stepwise selection, and backward stepwise selection have some limitations. They are computationally expensive and can lead to overfitting. Shrinkage methods are an alternative approach to subset selection. we fit a model containing all p predictors using a technique that constrains or regularizes the coefficient estimates, or equivalently, that shrinks the coefficient estimates towards zero. It may not be immediately obvious why such a constraint should improve the fit, but it turns out that shrinking the coefficient estimates can significantly reduce their variance. The two best-known techniques for shrinking the regression coefficients towards zero are ridge regression and the lasso.\n",
        "\n",
        "-   In logistic regression, for ridge regression (l-2 penalty), we need to optimize the following objective function:\n",
        "\n",
        "$$\n",
        "\\ell^*(\\boldsymbol\\beta) = \\ell(\\boldsymbol\\beta) - \\lambda \\sum_{j=1}^{p} \\beta_j^2,\n",
        "$$\n",
        "  where the penalty term is $\\lambda \\sum_{j=1}^{p} \\beta_j^2$.\n",
        "\n",
        "-   For the lasso (l-1 penalty), we need to optimize the following objective function:\n",
        "\n",
        "$$\n",
        "\\ell^*(\\boldsymbol\\beta) = \\ell(\\boldsymbol\\beta) - \\lambda \\sum_{j=1}^{p} |\\beta_j|,\n",
        "$$\n",
        "  where the penalty term is $\\lambda \\sum_{j=1}^{p} |\\beta_j|$. \n",
        "\n",
        "-   The elastic net combines the ridge and lasso penalties, and the penalty term is $\\lambda \\left( \\alpha \\sum_{j=1}^{p} |\\beta_j| + \\frac{1 - \\alpha}{2} \\sum_{j=1}^{p} \\beta_j^2 \\right)$, where $\\alpha$ controls the relative weight of the two penalties.\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"./shrink.jpg\" width=700>\n",
        "</p>\n",
        "\n",
        "Implementing ridge regression and the lasso requires a method for selecting a value for the tuning parameter $\\lambda$ (and $\\alpha$ if we use elastic net). Cross-validation provides a simple way to tackle this problem. We choose a grid of tuning parameters values, and compute the cross-validation error for each value of tuning parameters. We then select the tuning parameters value for which the cross-validation error is smallest. Finally, the model is re-fit using all of the available observations and the selected value of the tuning parameter.\n",
        "\n",
        "\n",
        "## Logistic regression (with enet regularization) workflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "\n",
        "library(tidymodels)\n",
        "library(gtsummary)\n",
        "\n",
        "data_clean <- fss_21_data |>\n",
        "  select(HRFS12M1, HRNUMHOU, HRHTYPE, GEREG, PRCHLD, HRPOOR,\n",
        "         PRTAGE, PEEDUCA, PEMLR, RACE, HHSUPWGT) |>\n",
        "  mutate(HRFS12M1 = ifelse(HRFS12M1 == \"Food Security\",\n",
        "                           \"Food Security\", \"Food Insecurity\"))\n",
        "\n",
        "data_clean |>\n",
        "  tbl_summary(by = HRFS12M1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initial split into test and non-test sets\n",
        "\n",
        "We randomly split the data into 25% test data and 75% non-test data. Stratify on food security status."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "\n",
        "# For reproducibility\n",
        "set.seed(2024)\n",
        "\n",
        "data_split <- initial_split(\n",
        "  data_clean, \n",
        "  # stratify by HRFS12M1\n",
        "  strata = \"HRFS12M1\", \n",
        "  prop = 0.75\n",
        "  )\n",
        "data_split\n",
        "\n",
        "data_other <- training(data_split)\n",
        "dim(data_other)\n",
        "\n",
        "data_test <- testing(data_split)\n",
        "dim(data_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Recipe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "\n",
        "recipe <- \n",
        "  recipe(\n",
        "    HRFS12M1 ~ .,\n",
        "    data = data_other\n",
        "  ) |>\n",
        "  # remove the weights\n",
        "  step_rm(HHSUPWGT) |>\n",
        "  # create dummy variables for categorical predictors\n",
        "  step_dummy(all_nominal_predictors()) |>\n",
        "  # zero-variance filter\n",
        "  step_zv(all_numeric_predictors()) |> \n",
        "  # center and scale numeric data\n",
        "  step_normalize(all_numeric_predictors()) |>\n",
        "  # estimate the means and standard deviations\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "\n",
        "logit_mod <- \n",
        "  logistic_reg(\n",
        "    penalty = tune(), \n",
        "    mixture = tune()\n",
        "  ) |> \n",
        "  set_engine(\"glmnet\", standardize = FALSE) |>\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Workflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "\n",
        "train_weight <- round(data_other$HHSUPWGT / 1000, 0)\n",
        "train_weight <- ifelse(train_weight == 0, 1, train_weight)\n",
        "\n",
        "\n",
        "logit_wf <- workflow() |>\n",
        "  #add_case_weights(train_weight) |>\n",
        "  add_recipe(recipe) |>\n",
        "  add_model(logit_mod) |>\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tuning grid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "\n",
        "param_grid <- grid_regular(\n",
        "  penalty(range = c(-6, 3)), \n",
        "  mixture(),\n",
        "  levels = c(100, 5)\n",
        "  ) |>\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cross-validation (CV)\n",
        "\n",
        "Set cross-validation partitions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "\n",
        "set.seed(2024)\n",
        "\n",
        "folds <- vfold_cv(data_other, v = 5)\n",
        "folds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fit cross-validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "\n",
        "(logit_fit <- logit_wf |>\n",
        "  tune_grid(\n",
        "    resamples = folds,\n",
        "    grid = param_grid,\n",
        "    metrics = metric_set(roc_auc, accuracy)\n",
        "    )) |>\n",
        "  system.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize CV results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "\n",
        "logit_fit |>\n",
        "  # aggregate metrics from K folds\n",
        "  collect_metrics() |>\n",
        "  print(width = Inf) |>\n",
        "  filter(.metric == \"roc_auc\") |>\n",
        "  ggplot(mapping = aes(x = penalty, y = mean, color = factor(mixture))) +\n",
        "  geom_point() +\n",
        "  labs(x = \"Penalty\", y = \"CV AUC\") +\n",
        "  scale_x_log10()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Show the top 5 models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "\n",
        "logit_fit |>\n",
        "  show_best(metric = \"roc_auc\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let’s select the best model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "\n",
        "best_logit <- logit_fit |>\n",
        "  select_best(metric = \"roc_auc\")\n",
        "best_logit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Final model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Final workflow\n",
        "final_wf <- logit_wf |>\n",
        "  finalize_workflow(best_logit)\n",
        "final_wf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "\n",
        "# Fit the whole training set, then predict the test cases\n",
        "final_fit <- \n",
        "  final_wf |>\n",
        "  last_fit(data_split)\n",
        "final_fit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "\n",
        "# Test metrics\n",
        "final_fit |> \n",
        "  collect_metrics()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "ir43",
      "language": "R",
      "display_name": "R 4.3.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}